# -*- coding: utf-8 -*-
"""P3.ipynb

Automatically generated by Colaboratory.

@author: Miguel Lentisco Ballesteros
"""

import cv2 as cv2
import numpy as np


"""# Funciones auxiliares"""

def read_image(path, flag = cv2.IMREAD_COLOR, dtype = "float32"):
  """Carga la imagen de una ruta, con un flag y tipo indicados.

  Parameters
  ----------
  path : string
      La ruta donde se encuentra la imagen.
  flag : int
      Indica como se lee la imagen (color, grises...).
  dtype : numpy.dtype or string
      El tipo de los valores de la imagen.

  Returns
  -------
  numpy.array
      La imagen cargada.
  """

  img = cv2.imread(path, flag)
  return img.astype(dtype)

def show_image(title, img):
  """Imprime la imagen por consola.

  Parameters
  ----------
  title: string
      Título de la ventana.
  img : numpy.array
      La imagen que se desea imprimir de tipo uint8.
  """
  cv2.imshow(title, img)
  cv2.waitKey(0)
  cv2.destroyAllWindows()

def reescale_uint8(img):
  """Transforma el tipo del imagen a tipo uint8, normalizando sus valores.

  Parameters
  ----------
  img : numyp.array
      La imagen.

  Returns
  -------
  numpy.array
      La imagen copiada en formato uint8
  """
  # Pasamos a uint8 y normalizamos
  return cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)


"""# Ejercicio 1"""

def apply_gaussian(img, sigma):
  """Aplica a la imagen un filtro gaussiano

  Parameters
  ----------
  img : numpy.array
      La imagen.
  sigma : float
      El sigma aplicado para el filtro gaussiano.

  Returns
  -------
  numpy.array
      La imagen convolucionada con un filtro gaussiano.
  """
  # El tamaño del kernel
  ksize = round(3 * sigma) * 2 + 1
  # Obtenemos el kernel gaussiano (usamos el mismo para X e Y)
  gauss_ker = cv2.getGaussianKernel(ksize, sigma)
  # Aplicamos el filtro
  return cv2.sepFilter2D(img, -1, gauss_ker, gauss_ker)

def apply_derivative(img, dx, dy, ksize):
  """Obtiene la derivada de orden (dx,dy) de la imagen.

  Parameters
  ----------
  img : numpy.array
      La imagen.
  dx : int
      índice de derivada respecto coordenada X.
  dy : int
      índice de derivada respecto coordenada Y.
  ksize : int
      Tamaño de los kernels de Sobel.

  Returns
  -------
  numpy.array
      La imagen aplicada con los filtros de derivadas.
  """
  # Obtenemos los filtros de derivadas
  kx, ky = cv2.getDerivKernels(dx, dy, ksize, normalize = True)
  # Aplicamos los filtros con la imagen
  return cv2.sepFilter2D(img, -1, kx, ky)

def get_gradient(img, ksize):
  """Obtiene el gradiente de la imagen.

  Parameters
  ----------
  img : numpy.array
      La imagen.
  ksize : int
      Tamaño de los kernels de Sobel.

  Returns
  -------
  numpy.array
      La derivada de la imagen respecto X.
  numpy.array
      La derivada de la imagen respecto Y.
  """
  # Obtenemos las derivadas primeras de cada coord
  dx = apply_derivative(img, 1, 0, ksize)
  dy = apply_derivative(img, 0, 1, ksize)
  return dx, dy

def get_gaussian_pyr(img, pyr_levels):
  """Devuelve la pirámide gaussiana de la imagen.

  Parameters
  ----------
  img : numpy.array
      La imagen a la que realizar la pirámide gaussiana.
  pyr_levels : int
      Nº de niveles de la pirámide, con 0 devuelve la imagen original.

  Returns
  -------
  list
      Una lista con los niveles de la pirámide gaussiana.
  """
  # Empezamos con la img original
  pyr = [img]
  # Obtenemos tantos niveles como se digan
  for i in range(pyr_levels):
    pyr.append(cv2.pyrDown(pyr[i]))
  return pyr

def get_corner_strength(img, blockSize, ksize):
  """Obtiene el valor del "corner strength" (operador Harris) para cada
     punto de la imagen.

  Parameters
  ----------
  img : numpy.array
      La imagen
  blockSize : int
      Tamaño del vecindario (entorno).
  ksize: int
      Tamaño del kernel de las matrices Sobel.

  Returns
  -------
  numpy.array
      Una matriz con el valor del operador Harris para cada píxel de la imagen
  """
  # Aplicamos un filtro gaussiano con escala de integración 1.5
  img_g1 = apply_gaussian(img, 1.5)
  # Aplicamos un filtro gaussiano con escala de derivación 1.0
  img_g2 = apply_gaussian(img_g1, 1.0)
  # Obtenemos los valores propios de la matriz de covarianzas
  # de las derivadas del entorno para cada pixel
  eigenVals = cv2.cornerEigenValsAndVecs(img_g2, blockSize, ksize)[:, :, :2]
  # Obtenemos el operador Harris: lambda_1 * lambda_ 2 / (lambda_1 + lambda_2)
  eigen1 = eigenVals[:, :, 0]
  eigen2 = eigenVals[:, :, 1]
  eigenMul = eigen1 * eigen2;
  eigenSum = eigen1 + eigen2;
  # Ponemos a 0 cuando la suma sea 0 (ignoramos el punto)
  return np.divide(eigenMul, eigenSum, out = np.zeros_like(eigenMul),
                    where = eigenSum != 0)

def non_maximum_suppresion(img, window_size):
  """Aplica la supresión de no-máximos a la imagen.

  Parameters
  ----------
  img : numpy.array
      La imagen.
  window_size : int
      Tamaño de la ventana que se coge en cada píxel.

  Returns
  -------
  numpy.array
      Una imagen nueva con la supresión aplicada a la imagen original.
  """
  # Imagen resultado
  res = np.zeros_like(img)
  # Por cada píxel
  for i, j in np.ndindex(img.shape):
    # Radio ventana
    r = window_size // 2
    # Tomamos el entorno 3x3 del píxel (cuando se pueda)
    window = img[max(i - r, 0):min(i + r + 1, img.shape[0]),
                        max(j - r, 0):min(j + r + 1, img.shape[1])]
    # Añadimos el punto si es el máximo de su entorno
    if (np.max(window) == img[i, j]):
      res[i, j] = img[i, j]
  return res

def get_angles(img, index, ksize):
  """Obtiene la orientación en grados de cada píxel

  Parameters
  ----------
  img : numpy.array
      La imagen
  index : list, tuple, numpy.array
      Los índices de los que se quiere extraer la orientación
  ksize
      Tamaño del kernel de las matrices Sobel para el gradiente.

  Returns
  -------
  numpy.array
      Una matriz con los ángulos en grados de cada píxel de index.
  """
  # Obtenemos el gradiente de la imagen con filtro gaussiano de sigma = 4.5
  img_gaussian = apply_gaussian(img, 4.5)
  dx, dy = get_gradient(img_gaussian, ksize)
  # Nos quedamos con los índices que se indican
  dx = dx[index]
  dy = dy[index]
  # Obtenemos el ángulo (orientación)
  return cv2.phase(dx.flatten(), dy.flatten(), angleInDegrees = True)

def get_corrected_points(img, keypoints):
  """Obtiene las coord corregidas de cada keypoint.

  Parameters
  ----------
  img : numpy.array
      La imagen
  keypoints: numpy.array
      Array con los keypoints

  Returns
  -------
  numpy.array
      Coordenadas de los puntos corregidos de los keypoints
  """
  # Tamaño de ventana
  WINSIZE = (5, 5)
  # Región muerta- nada
  ZEROZONE = (-1, -1)
  # Criterio de parada
  CRITERIA = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS, 50, 0.01)
  # Obtenemos los subpixels corregidos de los keypoints
  corrected_points = keypoints.astype("float32").reshape(-1, 2)
  return cv2.cornerSubPix(img, corrected_points, WINSIZE, ZEROZONE, CRITERIA)

def show_corrected_corners(img, keypoints, corrected_points):
  """Muestra tres entorno de 10x10 aumentado x5 de 3 zonas donde no coincidan
     los keypoints y los corregidos.

  Parameters
  ----------
  img : numpy.array
      La imagen
  keypoints: numpy.array
      Keypoints obtenidos
  corrected_points: numpy.arrray
      Keypoints corregidos
  """
  # Zoom x5
  ZOOM = 5
  # Puntos elegidos
  chosen_points = []
  # Hasta que tengamos 3
  index = np.arange(corrected_points.shape[0])
  while len(chosen_points) < 3:
      # Escogemos un índice aleatorio
      i = np.random.choice(index, 1, replace = False)
        #print(corrected_points[i, 0], keypoints[i, 0])
      if (corrected_points[i, 0] != keypoints[i, 0] or
            corrected_points[i, 1] != keypoints[i, 1]):
            chosen_points.append(i)
  # Para cada difernencia
  img_orig = img
  for p_index in chosen_points:
      # Cogemos los puntos
      x_k = int(keypoints[p_index, 0])
      y_k = int(keypoints[p_index, 1])
      x_c = int(corrected_points[p_index, 0])
      y_c = int(corrected_points[p_index, 1])
      # Para color
      img = cv2.cvtColor(img_orig, cv2.COLOR_GRAY2BGR)
      # Hacemos zoom x5
      img = cv2.resize(img, None, fx = ZOOM, fy = ZOOM)
      # Pintamos un círculo rojo en el punto original
      img = cv2.circle(img, (ZOOM * y_k, ZOOM * x_k), 4, (0, 0, 255))
      # Pintamos un círculo verde en el punto corregido
      img = cv2.circle(img, (ZOOM * y_c, ZOOM * x_c), 4, (0, 255, 0))
      # Imprimimos el entorno 10x10 del punto original
      window = img[max(ZOOM * (x_k - 5), 0):min(ZOOM * (x_k + 5),
                   ZOOM * img.shape[0]), max(ZOOM * (y_k - 5), 0) :
                   min(ZOOM * (y_k + 5), ZOOM * img.shape[1])]
      # Mostramos
      show_image("", reescale_uint8(window))

def harris_points_detector(img, pyr_levels):
  """ Dibuja los puntos Harris detecados a distintas escalas.

  Parameters
  ----------
  img : numpy.array
      La imagen.
  pyr_levels : int
      Nº de niveles de la pirámide gaussiana (0 es solo la imagen original).
  """
  # Umbral
  THRESHOLD = 10.0
  # Tamaño kernel
  KSIZE = 3
  # Tamaño bloque eigenvalues
  BLOCKSIZE = 3
  # Tamaño ventana supresión
  WINDOW_SIZE = 3
  # Img uint8
  img_uint8 = reescale_uint8(img)
  # Hacemos la pirámide gaussiana
  gaussian_pyr = get_gaussian_pyr(img, pyr_levels)
  # Obtenemos los puntos para cada nivel de la pirámide
  for i in range(pyr_levels + 1):
    # Imagen actual
    img_i = gaussian_pyr[i]
    # Aplicamos el operador Harris (corner strength)
    interest_points = get_corner_strength(img_i, BLOCKSIZE, KSIZE)
    # Aplicamos thresholding
    interest_points[interest_points < THRESHOLD] = 0.0
    # Aplicamos supresión de no-máximos
    interest_points = non_maximum_suppresion(interest_points, WINDOW_SIZE)
    # Obtenemos la escala: blockSize * (pyr_level + 1)
    scale = BLOCKSIZE * (i + 1)
    # Índices de los puntos sin suprimir
    points_index = np.nonzero(interest_points)
    # Obtenemos la orientación de cada punto
    angles = get_angles(img_i, points_index, KSIZE)
    # Formamos los keypoints
    keypoints = []
    for y, x, angle in zip(points_index[0], points_index[1], angles):
      keypoints.append(cv2.KeyPoint(x * 2 ** i, y * 2 ** i, scale, angle))
    # Coordenadas (X,Y)
    keypoints_scale = np.uint8([p.pt[::-1] for p in keypoints])
    # Nº de puntos por nivel
    print("Nivel", i, ", nº de puntos:", len(keypoints))
    # Dibujamos los keypoints
    image_points = cv2.drawKeypoints(img_uint8, keypoints, None,
                       color = (0, 0, 255),
                       flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
    # Imprimimos el resultado
    show_image("Harris points", image_points)

    # Puntos corregidos
    corrected_points = get_corrected_points(img, keypoints_scale)
    # Mostramos las diferencias
    show_corrected_corners(img, keypoints_scale, corrected_points)

def ej1():
  print("EJERCICIO1: Detector de Puntos Harris")
  yosemite1 = "./imgs/yosemite/Yosemite1.jpg"
  tablero1 = "./imgs/tablero/Tablero1.jpg"
  paths = [yosemite1, tablero1]
  for path in paths:
    img = read_image(path, cv2.IMREAD_GRAYSCALE)
    print("Harris detector, 4 levels")
    harris_points_detector(img, 4)

"""# Ejercicio 2"""

def get_keypoints_descriptors_akaze(img):
  """ Devuelve los keypoints y descriptores AKAZE de la imagen.

  Parameters
  ----------
  img : numpy.array
      La imagen

  Returns
  -------
  numpy.array
     Array con los keypoints AKAZE de la imagen.
  numpy.array
     Array con los descriptores AKAZE de la imagen.
  """
  # Creamos AKAZE
  akaze = cv2.AKAZE_create()
  # Obtenemos keypoints y descriptores
  return akaze.detectAndCompute(img, None)

def match_brute_force_cross_check(desc1, desc2):
  """ Devuelve los matches de dos imagenes con sus descriptores,
      mediante Brute-force + CrossCheck.

  Parameters
  ----------
  desc1 : numpy.array
      Descriptores AKAZE de la imagen1
  desc2 : numpy.array
      Descriptores AKAZE de la imagen2

  Returns
  -------
  numpy.array
     Array con los matches entre las dos imagenes.
  """
  # Creamos BF con norma Hamming (AKAZE da descriptores binarios) y CC activado
  bf = cv2.BFMatcher_create(cv2.NORM_HAMMING, crossCheck = True)
  # Hacemos los matches
  return bf.match(desc1, desc2)

def match_lowe_average_2nn(desc1, desc2):
  """ Devuelve los matches de dos imagenes con sus descriptores,
      mediante criterio Lowe-Average-2NN.

  Parameters
  ----------
  desc1 : numpy.array
      Descriptores AKAZE de la imagen1
  desc2 : numpy.array
      Descriptores AKAZE de la imagen2

  Returns
  -------
  numpy.array
     Array con los matches entre las dos imagenes.
  """
  # Creamos BF con norma Hamming
  bf = cv2.BFMatcher_create(cv2.NORM_HAMMING)
  # Hacemos matches con knn, k = 2
  matches = bf.knnMatch(desc1, desc2, k = 2)
  # Aplicamos ratio test
  good_matches = []
  for m,n in matches:
    if m.distance < 0.75 * n.distance:
      good_matches.append(m)
  return good_matches

def draw_matches_akaze(img1, img2, bf_match_method):
  """ Muestra los matches entre dos imagenes mediante
      descriptor AKAZE y con un método BruteForce dado.

  Parameters
  ----------
  img1 : numpy.array
      Imagen 1
  img2 : numpy.array
      Imagen 2
  bf_match_method : function(AKAZE descriptor, AKAZE descriptor)
      Método BruteForce, toma descriptores y devuelve los matches.
  """
  # Obtenemos los keypoints y descriptores AKAZE
  kpts1, desc1 = get_keypoints_descriptors_akaze(img1)
  kpts2, desc2 = get_keypoints_descriptors_akaze(img2)
  # Obtenemos los matches con el método dado
  matches = bf_match_method(desc1, desc2)
  # Tomamos 100 matches aleatorios)
  matches = np.random.choice(matches, min(len(matches), 100), replace = False)
  # Mostramos los matches
  img_matches = cv2.drawMatches(img1, kpts1, img2, kpts2, matches, None,
                        flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)
  show_image("Matching imgs", img_matches)

def ej2():
  # Imagenes
  yosemite1 = "./imgs/yosemite/Yosemite1.jpg"
  yosemite2 = "./imgs/yosemite/Yosemite2.jpg"

  tablero1 = "./imgs/tablero/Tablero1.jpg"
  tablero2 = "./imgs/tablero/Tablero2.jpg"

  pairs = [(yosemite1, yosemite2), (tablero1, tablero2)]
  modos = [cv2.IMREAD_GRAYSCALE, cv2.IMREAD_COLOR]
  for pair in pairs:
      for modo in modos:
          # Leemos imagenes
          img1 = read_image(pair[0], modo, "uint8")
          img2 = read_image(pair[1], modo, "uint8")
          # BF + CC
          print("Brute-force + CrossCheck")
          draw_matches_akaze(img1, img2, match_brute_force_cross_check)
          # Lowe-Average-2nn
          print("Lowe-Average-2nn")
          draw_matches_akaze(img1, img2, match_lowe_average_2nn)

"""# Ejercicio 3"""

def get_corners(img, offset_x = 0, offset_y = 0):
  """ Devuelve un vector de coordenadas de las esquinas
      de la imagen.

  Parameters
  ----------
  desc1 : numpy.array
      Descriptores AKAZE de la imagen1
  offset_x : int
      Desplazamiento en el eje X
  offset_y : int
      Desplazamiento en el eje Y

  Returns
  -------
  numpy.array
     Array con los matches entre las dos imagenes.
  """
  return np.array([(offset_x, offset_y), (offset_x, img.shape[1] + offset_y),
                     (offset_x + img.shape[0], offset_y),
                     (offset_x + img.shape[0], offset_y + img.shape[1])])

def create_canvas(img):
  """ Crea un lienzo con fondo negro y proyecta la imagen cerca del centro.
      Además devuelve la homografía usada para proyectar al lienzo.

  Parameters
  ----------
  img : numpy.array
      La imagen central.

  Returns
  -------
  numpy.array
     Lienzo con la imagen en el centro.
  numpy.array
     Matriz de la homografía para el lienzo.
  """
  # Nº de canales
  channels = 3 if len(img.shape) > 2 else 1
  # Creamos el lienzo
  (h, w, c) = (round(img.shape[0] * 2.2), round(img.shape[1] * 3.2), channels)
  canvas = np.zeros((h, w, c), dtype = "uint8")
  # Obtenemos los puntos para hacer la homografía
  puntos_img = get_corners(img)
  puntos_canvas = get_corners(img, w // 2.7, h // 4)
  # Hacemos la homografía
  H_canvas = cv2.findHomography(puntos_img, puntos_canvas, cv2.RANSAC)[0]
  # Proyectamos la imagen al centro
  canvas = cv2.warpPerspective(img, H_canvas, (w, h), dst = canvas,
                                    borderMode = cv2.BORDER_TRANSPARENT)
  return canvas, H_canvas

def get_homography(img1, img2, bf_match_method):
  """ Devuelve una homografía para proyectar img2 en img1.
      Se usan los descriptores AKAZE y matching el que se proporcione.

  Parameters
  ----------
  img1 : numpy.array
      Imagen donde se quiere proyectar.
  img2 : numpy.array
      Imagen desde donde se quiere proyectar.
  bf_match_method: funcion(AKAZE descriptor, AKAZE descriptor)
      Método para hacer match entre descriptores

  Returns
  -------
  numpy.array
     Matriz de la homografía.
  """
  # Extraemos los keypoints y descriptores AKAZE
  kpts1, desc1 = get_keypoints_descriptors_akaze(img1)
  kpts2, desc2 = get_keypoints_descriptors_akaze(img2)
  # Obtenemos los matches con el metodo proporcionado
  matches = bf_match_method(desc1, desc2)
  # Sacamos las coordenadas de los matches en cada imagen
  scene = np.float32([kpts1[match.queryIdx].pt for match in matches])
  obj = np.float32([kpts2[match.trainIdx].pt for match in matches])
  # Estimamos la homografía
  return cv2.findHomography(obj, scene, cv2.RANSAC, 1)[0]

def draw_2_mosaic(img1, img2, bf_match_method):
  """ Dibuja un mosaico con dos imagenes.

  Parameters
  ----------
  img1 : numpy.array
      Imagen donde se quiere proyectar.
  img2 : numpy.array
      Imagen desde donde se quiere proyectar.
  bf_match_method: funcion(AKAZE descriptor, AKAZE descriptor)
      Método para hacer match entre descriptores
  """
  # Creamos el canvas donde pondremos nuestro mosaico
  canvas, H_canvas = create_canvas(img1)
  (h, w) = canvas.shape[:2]
  # Obtenemos la homografía
  H = get_homography(img1, img2, bf_match_method)
  # Juuntamos img2 en el lienzo
  canvas = cv2.warpPerspective(img2, np.dot(H_canvas, H), (w, h), dst = canvas,
                                   borderMode = cv2.BORDER_TRANSPARENT)
  # Mostramos
  show_image("Mosaico", canvas)

def ej3():
  print("EJERCICIO 3: Mosaicos con 2 imágenes.")

  yosemite1 = "./imgs/yosemite/Yosemite1.jpg"
  yosemite2 = "./imgs/yosemite/Yosemite2.jpg"


  print("Yosemite grises")
  img1 = read_image(yosemite1, cv2.IMREAD_GRAYSCALE, "uint8")
  img2 = read_image(yosemite2, cv2.IMREAD_GRAYSCALE, "uint8")
  draw_2_mosaic(img1, img2, match_lowe_average_2nn)

  print("Yosemite colores")
  img1 = read_image(yosemite1, cv2.IMREAD_COLOR, "uint8")
  img2 = read_image(yosemite2, cv2.IMREAD_COLOR, "uint8")
  draw_2_mosaic(img1, img2, match_brute_force_cross_check)

"""# Ejercicio 4"""

def draw_full_mosaic(imgs, bf_match_method):
  """ Muestra el mosaico formado por varias imágenes. Se asume
      que las imagenes están ordenadas de izquierda a derecha.

  Parameters
  ----------
  imgs : list
      Lista de imagenes ordenadas de izq a der.
  bf_match_method : function(AKAZE descriptor, AKAZE descriptor)
      Método BruteForce, toma descriptores y devuelve los matches.
  """
  # Tomamos el centro del mosaico
  center_index = len(imgs) // 2
  img_centro = imgs[center_index]
  # Formamos el canvas con él
  canvas, H_canvas = create_canvas(img_centro)
  (h, w) = canvas.shape[:2]
  # Obtenemos las homográfias entre cada par de imagenes
  homographies = []
  # Parte izquierda
  for i in range(center_index):
      homographies.append(get_homography(imgs[i+1], imgs[i], bf_match_method))
  # Parte derecha
  for i in range(center_index, len(imgs) - 1):
      homographies.append(get_homography(imgs[i], imgs[i+1], bf_match_method))
  # Parte derecha
  H_right = np.copy(H_canvas)
  for i in range(center_index, len(imgs) - 1):
    # Composición
    H_right = np.dot(H_right, homographies[i])
    # Uno
    canvas = cv2.warpPerspective(imgs[i + 1], H_right, (w, h), dst = canvas,
                                  borderMode = cv2.BORDER_TRANSPARENT)
  # Parte izquierda
  H_left = np.copy(H_canvas)
  for i in range(center_index, 0, -1):
    # Composición
    H_left = np.dot(H_left, homographies[i - 1])
    # Uno
    canvas = cv2.warpPerspective(imgs[i - 1], H_left, (w, h), dst = canvas,
                                  borderMode = cv2.BORDER_TRANSPARENT)

  # Imprimimos el mosaico
  show_image("Full mosaic", canvas)


def ej4():
    print("EJERCICIO 4: Mosaicos de varias imágenes.")

    y1 = "imgs/yosemite_full/yosemite1.jpg"
    y2 = "imgs/yosemite_full/yosemite2.jpg"
    y3 = "imgs/yosemite_full/yosemite3.jpg"
    y4 = "imgs/yosemite_full/yosemite4.jpg"
    y5 = "imgs/yosemite_full/yosemite5.jpg"
    y6 = "imgs/yosemite_full/yosemite6.jpg"
    y7 = "imgs/yosemite_full/yosemite7.jpg"
    yos1 = [y1, y2, y3, y4]
    yos2 = [y5, y6, y7]

    et1 = "imgs/mosaico-etsiit/mosaico002.jpg"
    et2 = "imgs/mosaico-etsiit/mosaico003.jpg"
    et3 = "imgs/mosaico-etsiit/mosaico004.jpg"
    et4 = "imgs/mosaico-etsiit/mosaico005.jpg"
    et5 = "imgs/mosaico-etsiit/mosaico006.jpg"
    et6 = "imgs/mosaico-etsiit/mosaico007.jpg"
    et7 = "imgs/mosaico-etsiit/mosaico008.jpg"
    et8 = "imgs/mosaico-etsiit/mosaico009.jpg"
    et9 = "imgs/mosaico-etsiit/mosaico010.jpg"
    et10 = "imgs/mosaico-etsiit/mosaico011.jpg"
    etsiit = [et1, et2, et3, et4, et5, et6, et7, et8, et9, et10]

    mosaics = [yos2, yos1, etsiit]
    # Para cada mosaico lo dibujamos en grises y color
    for mosaic in mosaics:
        imgs_g = []
        imgs_c = []
        # Leemos las imagenes
        for path in mosaic:
            imgs_g.append(read_image(path, cv2.IMREAD_GRAYSCALE, "uint8"))
            imgs_c.append(read_image(path, cv2.IMREAD_COLOR, "uint8"))
        # Dibujamos los mosaicos
        print("Mosaico completo grises")
        draw_full_mosaic(imgs_g, match_lowe_average_2nn)
        print("Mosaico completo colores")
        draw_full_mosaic(imgs_c, match_lowe_average_2nn)


def main():
    ej1()
    ej2()
    ej3()
    ej4()
main()
